{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Deep Learning Homework 5 & 6\n",
    "\n",
    "## Problem 2 ONLY\n",
    "\n",
    "**Nathan Inkawhich**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Duke Community Standard](http://integrity.duke.edu/standard.html): By typing your name below, you are certifying that you have adhered to the Duke Community Standard in completing this assignment.**\n",
    "\n",
    "Name: Nathan Inkawhich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2:  Recurrent Neural Networks (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import string\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and format word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word vectors\n",
    "if not os.path.isfile('mini.h5'):\n",
    "    print(\"Downloading Conceptnet Numberbatch word embeddings...\")\n",
    "    conceptnet_url = 'http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'\n",
    "    urlretrieve(conceptnet_url, 'mini.h5')\n",
    "    \n",
    "# Decode file\n",
    "with h5py.File('mini.h5', 'r') as f:\n",
    "    all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]]\n",
    "    all_embeddings = f['mat']['block0_values'][:]\n",
    "    \n",
    "# Extract English words\n",
    "english_words = [word[6:] for word in all_words if word.startswith('/c/en/')]\n",
    "english_word_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en/')]\n",
    "english_embedddings = all_embeddings[english_word_indices]\n",
    "\n",
    "# Normalize Embeddings to unit circle\n",
    "norms = np.linalg.norm(english_embedddings, axis=1)\n",
    "normalized_embeddings = english_embedddings.astype('float32') / norms.astype('float32').reshape([-1, 1])\n",
    "\n",
    "# Create LUT\n",
    "index = {word: i for i, word in enumerate(english_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(w1, w2):\n",
    "    score = np.dot(normalized_embeddings[index[w1], :], normalized_embeddings[index[w2], :])\n",
    "    return score\n",
    "\n",
    "def print_similarity(w1,w2):\n",
    "    try:\n",
    "        print('{0}\\t{1}\\t'.format(w1,w2), \\\n",
    "          similarity_score('{}'.format(w1), '{}'.format(w2)))\n",
    "    except:\n",
    "        print('One of the words is not in the dictionary.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\tcat\t 1.0000001\n",
      "cat\tfeline\t 0.8199548\n",
      "cat\tdog\t 0.590724\n",
      "cat\tmoo\t 0.0039538303\n",
      "cat\tfreeze\t -0.030225191\n"
     ]
    }
   ],
   "source": [
    "# A word is as similar with itself as possible:\n",
    "print('cat\\tcat\\t', similarity_score('cat', 'cat'))\n",
    "# Closely related words still get high scores:\n",
    "print('cat\\tfeline\\t', similarity_score('cat', 'feline'))\n",
    "print('cat\\tdog\\t', similarity_score('cat', 'dog'))\n",
    "# Unrelated words, not so much\n",
    "print('cat\\tmoo\\t', similarity_score('cat', 'moo'))\n",
    "print('cat\\tfreeze\\t', similarity_score('cat', 'freeze'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare movie dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punct=str.maketrans('','',string.punctuation)\n",
    "\n",
    "# This function converts a line of our data file into\n",
    "# a tuple (x, y), where x is 300-dimensional representation\n",
    "# of the words in a review, and y is its label.\n",
    "def convert_line_to_example(line):\n",
    "    # Pull out the first character: that's our label (0 or 1)\n",
    "    y = int(line[0])\n",
    "    # Split the line into words using Python's split() function\n",
    "    words = line[2:].translate(remove_punct).lower().split()\n",
    "    # Look up the embeddings of each word, ignoring words not\n",
    "    # in our pretrained vocabulary.\n",
    "    embeddings = [normalized_embeddings[index[w]] for w in words\n",
    "                  if w in index]\n",
    "    # Take the mean of the embeddings\n",
    "    x = np.mean(np.vstack(embeddings), axis=0)\n",
    "    return {'x': x, 'y': y, 'w':embeddings}\n",
    "\n",
    "# Apply the function to each line in the file.\n",
    "enc = 'utf-8' # This is necessary from within the singularity shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1: Train an MLP off of the average word embedding to predict sentiment (as done in class) but optimize the network settings to maximize performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset:  1411\n"
     ]
    }
   ],
   "source": [
    "### Choose Dataset\n",
    "with open(\"Data/movie-simple.txt\", \"r\", encoding=enc) as f:\n",
    "    dataset = [convert_line_to_example(l) for l in f.readlines()]  \n",
    "#with open(\"Data/movie-pang02.txt\", \"r\",encoding=enc) as f:\n",
    "#    dataset = [convert_line_to_example(l) for l in f.readlines()]\n",
    "\n",
    "print(\"Length of Dataset: \",len(dataset))\n",
    "# Shuffle full dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# Split full dataset into train/test splits\n",
    "batch_size = 100\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configs\n",
    "num_hidden_L1 = 100\n",
    "num_hidden_L2 = 100\n",
    "learning_rate = .01\n",
    "num_epochs = 2000\n",
    "\n",
    "# Clear all old tf graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders for input\n",
    "X = tf.placeholder(tf.float32, [None, 300]) # Word embedding size = 300\n",
    "y = tf.placeholder(tf.float32, [None, 1]) # Binary classification output: \"good\" or \"bad\"\n",
    "\n",
    "# Three-layer MLP\n",
    "h1 = tf.layers.dense(X, num_hidden_L1, tf.nn.relu)\n",
    "h2 = tf.layers.dense(h1, num_hidden_L2, tf.nn.relu)\n",
    "logits = tf.layers.dense(h2, 1)\n",
    "probabilities = tf.sigmoid(logits)\n",
    "\n",
    "# Loss and metrics\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(logits)), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Initialization of variables\n",
    "initialize_all = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.69180435 Acc 0.59\n",
      "Epoch 50 Loss 0.6843544 Acc 0.53\n",
      "Epoch 100 Loss 0.6723387 Acc 0.56\n",
      "Epoch 150 Loss 0.66936374 Acc 0.53\n",
      "Epoch 200 Loss 0.64258176 Acc 0.62\n",
      "Epoch 250 Loss 0.64455914 Acc 0.61\n",
      "Epoch 300 Loss 0.6015594 Acc 0.67\n",
      "Epoch 350 Loss 0.5527072 Acc 0.86\n",
      "Epoch 400 Loss 0.5074774 Acc 0.86\n",
      "Epoch 450 Loss 0.47337112 Acc 0.81\n",
      "Epoch 500 Loss 0.3932708 Acc 0.87\n",
      "Epoch 550 Loss 0.25771186 Acc 0.94\n",
      "Epoch 600 Loss 0.25898987 Acc 0.91\n",
      "Epoch 650 Loss 0.22260652 Acc 0.94\n",
      "Epoch 700 Loss 0.23537724 Acc 0.91\n",
      "Epoch 750 Loss 0.18386932 Acc 0.96\n",
      "Epoch 800 Loss 0.15567163 Acc 0.98\n",
      "Epoch 850 Loss 0.1284622 Acc 0.96\n",
      "Epoch 900 Loss 0.13323972 Acc 0.96\n",
      "Epoch 950 Loss 0.10479845 Acc 0.99\n",
      "Epoch 1000 Loss 0.14932638 Acc 0.91\n",
      "Epoch 1050 Loss 0.1106052 Acc 0.99\n",
      "Epoch 1100 Loss 0.1391635 Acc 0.94\n",
      "Epoch 1150 Loss 0.0989832 Acc 0.98\n",
      "Epoch 1200 Loss 0.06615176 Acc 0.99\n",
      "Epoch 1250 Loss 0.07240047 Acc 0.98\n",
      "Epoch 1300 Loss 0.07415715 Acc 0.97\n",
      "Epoch 1350 Loss 0.09513224 Acc 0.97\n",
      "Epoch 1400 Loss 0.07491414 Acc 0.98\n",
      "Epoch 1450 Loss 0.08503489 Acc 0.97\n",
      "Epoch 1500 Loss 0.06658972 Acc 0.98\n",
      "Epoch 1550 Loss 0.0629354 Acc 0.99\n",
      "Epoch 1600 Loss 0.10106768 Acc 0.98\n",
      "Epoch 1650 Loss 0.053452894 Acc 0.99\n",
      "Epoch 1700 Loss 0.030821955 Acc 1.0\n",
      "Epoch 1750 Loss 0.042177048 Acc 1.0\n",
      "Epoch 1800 Loss 0.080082886 Acc 0.97\n",
      "Epoch 1850 Loss 0.088921 Acc 0.96\n",
      "Epoch 1900 Loss 0.09523052 Acc 0.94\n",
      "Epoch 1950 Loss 0.06007345 Acc 0.97\n",
      "Final test accuracy: 0.96350366\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(initialize_all)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = [sample['x'] for sample in data]\n",
    "        labels  = [sample['y'] for sample in data]\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"Epoch\", epoch, \"Loss\", l, \"Acc\", acc)\n",
    "    random.shuffle(train)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_reviews = [sample['x'] for sample in test]\n",
    "test_labels  = [sample['y'] for sample in test]\n",
    "test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "print(\"Final test accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2:  Train a RNN from the word embeddings to predict sentiment (as done in class) and optimize the network settings to maximize performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset:  1411\n"
     ]
    }
   ],
   "source": [
    "### Choose Dataset\n",
    "with open(\"Data/movie-simple.txt\", \"r\", encoding=enc) as f:\n",
    "    dataset = [convert_line_to_example(l) for l in f.readlines()]  \n",
    "#with open(\"Data/movie-pang02.txt\", \"r\",encoding=enc) as f:\n",
    "#    dataset = [convert_line_to_example(l) for l in f.readlines()]\n",
    "\n",
    "print(\"Length of Dataset: \",len(dataset))  \n",
    "random.shuffle(dataset)\n",
    "batch_size = 1\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear old tf stuff\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Configs\n",
    "n_steps = None\n",
    "n_inputs = 300\n",
    "n_neurons = 100\n",
    "num_epochs = 200\n",
    "\n",
    "# Input placeholders\n",
    "X= tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y= tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# Build RNN\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(n_neurons,activation=tf.nn.tanh)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "last_cell_output=outputs[:,-1,:]\n",
    "y_=tf.layers.dense(last_cell_output,1)\n",
    "\n",
    "# Loss and metrics\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_)), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train RNN\n",
    "\n",
    "Note, the batch size here is one because we train on all words from a single review then update the parameters. Thus, training this RNN is much slower than the MLP above which has batch size of 100, so we train the RNN for less epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.3577238227463844 Acc 0.8453032523592714\n",
      "Epoch 10 Loss 0.3585691392808854 Acc 0.8390407310869633\n",
      "Epoch 20 Loss 0.27741738111790826 Acc 0.896732067573775\n",
      "Epoch 30 Loss 0.10115193868826249 Acc 0.969631141992521\n",
      "Epoch 40 Loss 0.009688287996244882 Acc 0.9999289409800682\n",
      "Epoch 50 Loss 0.0415519876323821 Acc 0.983600320172465\n",
      "Epoch 60 Loss 0.054384881471194114 Acc 0.9874136834038832\n",
      "Epoch 70 Loss 0.045912286230596615 Acc 0.9940838024085178\n",
      "Epoch 80 Loss 0.02079570656470188 Acc 0.9917239253468476\n",
      "Epoch 90 Loss 0.005779521988234355 Acc 0.9999985838818193\n",
      "Epoch 100 Loss 0.004128102227181615 Acc 0.9999963174525147\n",
      "Epoch 110 Loss 0.09741377409208155 Acc 0.9730761900724079\n",
      "Epoch 120 Loss 0.0033819765849131597 Acc 0.9999400624453558\n",
      "Epoch 130 Loss 0.0005055673678424009 Acc 0.9999999999999946\n",
      "Epoch 140 Loss 0.01644081386447104 Acc 0.9968513176139709\n",
      "Epoch 150 Loss 0.00016596590522660083 Acc 0.9999999999999946\n",
      "Epoch 160 Loss 0.01730864439500782 Acc 0.9935953077416854\n",
      "Epoch 170 Loss 0.006897801608203365 Acc 0.9978717602355849\n",
      "Epoch 180 Loss 0.004612685133902259 Acc 0.9993249561791374\n",
      "Epoch 190 Loss 0.003954110556796616 Acc 0.9998325128657921\n"
     ]
    }
   ],
   "source": [
    "initialize_all = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(initialize_all)\n",
    "l_ma=.74\n",
    "acc_ma=.5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = np.array([sample['w'] for sample in data]).reshape([1,-1,300])\n",
    "        labels  = np.array([sample['y'] for sample in data]).reshape([1,1])\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "        l_ma=.99*l_ma+(.01)*l\n",
    "        acc_ma=.99*acc_ma+(.01)*acc\n",
    "        #if (batch+1) % 100 == 0:\n",
    "        #    print(\"batch\", batch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch\", epoch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
    "    random.shuffle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9320113314447592\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_acc=0\n",
    "n=0\n",
    "for sample in test:\n",
    "    test_reviews = np.array([sample['w'] ]).reshape([1,-1,300])\n",
    "    test_labels  = np.array([sample['y']]).reshape([1,1])\n",
    "    test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "    test_acc += sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "    n+=1\n",
    "acc=test_acc/n \n",
    "print(\"Final accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3:  Encode each vocabulary word as a one-hot vector. Train an MLP on the average of the onehot vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build one hot embedding functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(english_words):  150875\n",
      "english_embedddings.shape:  (150875, 300)\n",
      "onehot_embeddings.shape:  (150875, 150875)\n",
      "Size of index dict:  150875\n"
     ]
    }
   ],
   "source": [
    "print(\"len(english_words): \", len(english_words))\n",
    "print(\"english_embedddings.shape: \", english_embedddings.shape)\n",
    "\n",
    "# Build onehot encoding scheme with an identity matrix\n",
    "onehot_embeddings = np.identity(len(english_words),dtype=np.float32)\n",
    "print(\"onehot_embeddings.shape: \", onehot_embeddings.shape)\n",
    "#print(np.sum(onehot_embeddings,axis=0))\n",
    "#print(np.sum(onehot_embeddings,axis=1))\n",
    "\n",
    "# Create LUT\n",
    "index = {word: i for i, word in enumerate(english_words)}\n",
    "print(\"Size of index dict: \", len(index.keys()))\n",
    "\n",
    "remove_punct=str.maketrans('','',string.punctuation)\n",
    "\n",
    "# This function converts a line of our data file into\n",
    "# a tuple (x, y), where x is 150875-dimensional one-hot representation\n",
    "# of the words in a review, and y is its label.\n",
    "def convert_line_to_example_onehot(line):\n",
    "    # Pull out the first character: that's our label (0 or 1)\n",
    "    y = int(line[0])\n",
    "    # Split the line into words using Python's split() function\n",
    "    words = line[2:].translate(remove_punct).lower().split()\n",
    "    # Look up the embeddings of each word, ignoring words not\n",
    "    # in our pretrained vocabulary.\n",
    "    embeddings = [onehot_embeddings[index[w]] for w in words if w in index]\n",
    "    # Take the mean of the embeddings\n",
    "    x = np.mean(np.vstack(embeddings), axis=0)\n",
    "    return {'x': x, 'y': y, 'w':embeddings}\n",
    "\n",
    "# Apply the function to each line in the file.\n",
    "enc = 'utf-8' # This is necessary from within the singularity shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train/test datasets for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset:  1411\n",
      "# train:  1000\n",
      "# test:  411\n"
     ]
    }
   ],
   "source": [
    "### Choose Dataset\n",
    "with open(\"Data/movie-simple.txt\", \"r\", encoding=enc) as f:\n",
    "    dataset = [convert_line_to_example_onehot(l) for l in f.readlines()]  \n",
    "#with open(\"Data/movie-pang02.txt\", \"r\",encoding=enc) as f:\n",
    "#    dataset = [convert_line_to_example_onehot(l) for l in f.readlines()]\n",
    "\n",
    "print(\"Length of Dataset: \",len(dataset))\n",
    "\n",
    "# Split full dataset into train/test splits\n",
    "random.shuffle(dataset)\n",
    "batch_size = 100\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]\n",
    "\n",
    "print(\"# train: \", len(train))\n",
    "print(\"# test: \", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configs\n",
    "num_hidden_L1 = 100\n",
    "num_hidden_L2 = 100\n",
    "learning_rate = .01\n",
    "num_epochs = 2000\n",
    "\n",
    "# Clear all old tf graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders for input\n",
    "X = tf.placeholder(tf.float32, [None, 150875]) # Word embedding size = 150875\n",
    "y = tf.placeholder(tf.float32, [None, 1]) # Binary classification output: \"good\" or \"bad\"\n",
    "\n",
    "# Three-layer MLP\n",
    "h1 = tf.layers.dense(X, num_hidden_L1, tf.nn.relu)\n",
    "h2 = tf.layers.dense(h1, num_hidden_L2, tf.nn.relu)\n",
    "logits = tf.layers.dense(h2, 1)\n",
    "probabilities = tf.sigmoid(logits)\n",
    "\n",
    "# Loss and metrics\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(logits)), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Initialization of variables\n",
    "initialize_all = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.6936411 Acc 0.42\n",
      "Epoch 10 Loss 0.6881182 Acc 0.57\n",
      "Epoch 20 Loss 0.6828872 Acc 0.6\n",
      "Epoch 30 Loss 0.6769927 Acc 0.63\n",
      "Epoch 40 Loss 0.6756306 Acc 0.63\n",
      "Epoch 50 Loss 0.6797851 Acc 0.58\n",
      "Epoch 60 Loss 0.68623453 Acc 0.53\n",
      "Epoch 70 Loss 0.6842681 Acc 0.53\n",
      "Epoch 80 Loss 0.6786213 Acc 0.57\n",
      "Epoch 90 Loss 0.68487173 Acc 0.52\n",
      "Epoch 100 Loss 0.66725016 Acc 0.63\n",
      "Epoch 110 Loss 0.6832987 Acc 0.52\n",
      "Epoch 120 Loss 0.67074126 Acc 0.59\n",
      "Epoch 130 Loss 0.690413 Acc 0.45\n",
      "Epoch 140 Loss 0.67591757 Acc 0.54\n",
      "Epoch 150 Loss 0.6781906 Acc 0.52\n",
      "Epoch 160 Loss 0.6841625 Acc 0.47\n",
      "Epoch 170 Loss 0.6625603 Acc 0.6\n",
      "Epoch 180 Loss 0.6673707 Acc 0.55\n",
      "Epoch 190 Loss 0.6705307 Acc 0.56\n",
      "Epoch 200 Loss 0.6753393 Acc 0.51\n",
      "Epoch 210 Loss 0.6619837 Acc 0.55\n",
      "Epoch 220 Loss 0.66405845 Acc 0.57\n",
      "Epoch 230 Loss 0.6465087 Acc 0.68\n",
      "Epoch 240 Loss 0.6541261 Acc 0.59\n",
      "Epoch 250 Loss 0.66265184 Acc 0.57\n",
      "Epoch 260 Loss 0.6629913 Acc 0.54\n",
      "Epoch 270 Loss 0.6474299 Acc 0.58\n",
      "Epoch 280 Loss 0.63664186 Acc 0.69\n",
      "Epoch 290 Loss 0.63364553 Acc 0.62\n",
      "Epoch 300 Loss 0.62817913 Acc 0.68\n",
      "Epoch 310 Loss 0.6259397 Acc 0.64\n",
      "Epoch 320 Loss 0.6495351 Acc 0.57\n",
      "Epoch 330 Loss 0.6238335 Acc 0.75\n",
      "Epoch 340 Loss 0.61603224 Acc 0.73\n",
      "Epoch 350 Loss 0.618763 Acc 0.76\n",
      "Epoch 360 Loss 0.59795475 Acc 0.81\n",
      "Epoch 370 Loss 0.58072793 Acc 0.89\n",
      "Epoch 380 Loss 0.586773 Acc 0.88\n",
      "Epoch 390 Loss 0.57493174 Acc 0.87\n",
      "Epoch 400 Loss 0.576485 Acc 0.84\n",
      "Epoch 410 Loss 0.5528891 Acc 0.86\n",
      "Epoch 420 Loss 0.5581057 Acc 0.84\n",
      "Epoch 430 Loss 0.5384951 Acc 0.86\n",
      "Epoch 440 Loss 0.5204694 Acc 0.88\n",
      "Epoch 450 Loss 0.5480901 Acc 0.8\n",
      "Epoch 460 Loss 0.48597205 Acc 0.86\n",
      "Epoch 470 Loss 0.48021743 Acc 0.93\n",
      "Epoch 480 Loss 0.5271133 Acc 0.85\n",
      "Epoch 490 Loss 0.45947033 Acc 0.9\n",
      "Epoch 500 Loss 0.49439362 Acc 0.84\n",
      "Epoch 510 Loss 0.45202777 Acc 0.89\n",
      "Epoch 520 Loss 0.44292167 Acc 0.9\n",
      "Epoch 530 Loss 0.40839088 Acc 0.94\n",
      "Epoch 540 Loss 0.39312866 Acc 0.9\n",
      "Epoch 550 Loss 0.41564977 Acc 0.86\n",
      "Epoch 560 Loss 0.37558982 Acc 0.9\n",
      "Epoch 570 Loss 0.37004888 Acc 0.94\n",
      "Epoch 580 Loss 0.35683158 Acc 0.92\n",
      "Epoch 590 Loss 0.32477593 Acc 0.94\n",
      "Epoch 600 Loss 0.32547048 Acc 0.94\n",
      "Epoch 610 Loss 0.33041993 Acc 0.91\n",
      "Epoch 620 Loss 0.30212644 Acc 0.94\n",
      "Epoch 630 Loss 0.33457768 Acc 0.9\n",
      "Epoch 640 Loss 0.3239122 Acc 0.91\n",
      "Epoch 650 Loss 0.32572556 Acc 0.9\n",
      "Epoch 660 Loss 0.32530147 Acc 0.91\n",
      "Epoch 670 Loss 0.3019455 Acc 0.91\n",
      "Epoch 680 Loss 0.27157128 Acc 0.92\n",
      "Epoch 690 Loss 0.2778016 Acc 0.94\n",
      "Epoch 700 Loss 0.2619886 Acc 0.96\n",
      "Epoch 710 Loss 0.2565277 Acc 0.95\n",
      "Epoch 720 Loss 0.2213092 Acc 0.95\n",
      "Epoch 730 Loss 0.247805 Acc 0.97\n",
      "Epoch 740 Loss 0.22329426 Acc 0.98\n",
      "Epoch 750 Loss 0.22101507 Acc 0.97\n",
      "Epoch 760 Loss 0.2154018 Acc 0.93\n",
      "Epoch 770 Loss 0.25864917 Acc 0.95\n",
      "Epoch 780 Loss 0.21103854 Acc 0.96\n",
      "Epoch 790 Loss 0.17883654 Acc 0.98\n",
      "Epoch 800 Loss 0.20035271 Acc 0.96\n",
      "Epoch 810 Loss 0.19500528 Acc 0.96\n",
      "Epoch 820 Loss 0.18290153 Acc 0.95\n",
      "Epoch 830 Loss 0.23509096 Acc 0.96\n",
      "Epoch 840 Loss 0.16182877 Acc 0.96\n",
      "Epoch 850 Loss 0.18323047 Acc 0.97\n",
      "Epoch 860 Loss 0.17681523 Acc 0.96\n",
      "Epoch 870 Loss 0.1662325 Acc 0.99\n",
      "Epoch 880 Loss 0.13883118 Acc 0.96\n",
      "Epoch 890 Loss 0.16722858 Acc 0.97\n",
      "Epoch 900 Loss 0.15501955 Acc 0.98\n",
      "Epoch 910 Loss 0.17171797 Acc 0.97\n",
      "Epoch 920 Loss 0.14220089 Acc 0.98\n",
      "Epoch 930 Loss 0.18312222 Acc 0.93\n",
      "Epoch 940 Loss 0.15024473 Acc 0.98\n",
      "Epoch 950 Loss 0.13429667 Acc 0.98\n",
      "Epoch 960 Loss 0.12919086 Acc 0.97\n",
      "Epoch 970 Loss 0.12978713 Acc 0.97\n",
      "Epoch 980 Loss 0.14317119 Acc 0.98\n",
      "Epoch 990 Loss 0.16431841 Acc 0.96\n",
      "Epoch 1000 Loss 0.11689448 Acc 0.95\n",
      "Epoch 1010 Loss 0.1509321 Acc 0.97\n",
      "Epoch 1020 Loss 0.124143735 Acc 0.98\n",
      "Epoch 1030 Loss 0.11579497 Acc 0.99\n",
      "Epoch 1040 Loss 0.12783603 Acc 0.98\n",
      "Epoch 1050 Loss 0.12205724 Acc 0.98\n",
      "Epoch 1060 Loss 0.14551395 Acc 0.97\n",
      "Epoch 1070 Loss 0.10109132 Acc 1.0\n",
      "Epoch 1080 Loss 0.08700414 Acc 1.0\n",
      "Epoch 1090 Loss 0.10343229 Acc 0.97\n",
      "Epoch 1100 Loss 0.11405859 Acc 0.98\n",
      "Epoch 1110 Loss 0.13432877 Acc 0.94\n",
      "Epoch 1120 Loss 0.14056134 Acc 0.96\n",
      "Epoch 1130 Loss 0.10616148 Acc 0.99\n",
      "Epoch 1140 Loss 0.09481314 Acc 0.99\n",
      "Epoch 1150 Loss 0.08539056 Acc 1.0\n",
      "Epoch 1160 Loss 0.11345317 Acc 0.98\n",
      "Epoch 1170 Loss 0.125186 Acc 0.98\n",
      "Epoch 1180 Loss 0.10007351 Acc 1.0\n",
      "Epoch 1190 Loss 0.11238299 Acc 0.99\n",
      "Epoch 1200 Loss 0.07890668 Acc 1.0\n",
      "Epoch 1210 Loss 0.0879582 Acc 0.98\n",
      "Epoch 1220 Loss 0.07215698 Acc 1.0\n",
      "Epoch 1230 Loss 0.09223593 Acc 0.99\n",
      "Epoch 1240 Loss 0.069197975 Acc 0.98\n",
      "Epoch 1250 Loss 0.08570256 Acc 0.99\n",
      "Epoch 1260 Loss 0.09159474 Acc 0.98\n",
      "Epoch 1270 Loss 0.107155524 Acc 0.98\n",
      "Epoch 1280 Loss 0.05525567 Acc 1.0\n",
      "Epoch 1290 Loss 0.055849127 Acc 1.0\n",
      "Epoch 1300 Loss 0.06893315 Acc 1.0\n",
      "Epoch 1310 Loss 0.07647733 Acc 1.0\n",
      "Epoch 1320 Loss 0.080417976 Acc 0.98\n",
      "Epoch 1330 Loss 0.056681886 Acc 1.0\n",
      "Epoch 1340 Loss 0.06996559 Acc 1.0\n",
      "Epoch 1350 Loss 0.056941856 Acc 0.99\n",
      "Epoch 1360 Loss 0.06650716 Acc 1.0\n",
      "Epoch 1370 Loss 0.08827932 Acc 0.98\n",
      "Epoch 1380 Loss 0.06584824 Acc 1.0\n",
      "Epoch 1390 Loss 0.08239922 Acc 0.98\n",
      "Epoch 1400 Loss 0.061473325 Acc 0.99\n",
      "Epoch 1410 Loss 0.07711706 Acc 0.99\n",
      "Epoch 1420 Loss 0.067246854 Acc 0.99\n",
      "Epoch 1430 Loss 0.05783193 Acc 0.97\n",
      "Epoch 1440 Loss 0.06525933 Acc 1.0\n",
      "Epoch 1450 Loss 0.04814404 Acc 1.0\n",
      "Epoch 1460 Loss 0.054332603 Acc 1.0\n",
      "Epoch 1470 Loss 0.101206616 Acc 0.96\n",
      "Epoch 1480 Loss 0.07852976 Acc 0.99\n",
      "Epoch 1490 Loss 0.0443292 Acc 1.0\n",
      "Epoch 1500 Loss 0.061398536 Acc 0.99\n",
      "Epoch 1510 Loss 0.058687363 Acc 1.0\n",
      "Epoch 1520 Loss 0.05985539 Acc 0.99\n",
      "Epoch 1530 Loss 0.059569422 Acc 0.99\n",
      "Epoch 1540 Loss 0.060659565 Acc 0.98\n",
      "Epoch 1550 Loss 0.04763371 Acc 1.0\n",
      "Epoch 1560 Loss 0.0743561 Acc 0.99\n",
      "Epoch 1570 Loss 0.048473787 Acc 1.0\n",
      "Epoch 1580 Loss 0.043446712 Acc 1.0\n",
      "Epoch 1590 Loss 0.036338806 Acc 1.0\n",
      "Epoch 1600 Loss 0.033200253 Acc 1.0\n",
      "Epoch 1610 Loss 0.037659455 Acc 1.0\n",
      "Epoch 1620 Loss 0.058130227 Acc 0.99\n",
      "Epoch 1630 Loss 0.053065382 Acc 1.0\n",
      "Epoch 1640 Loss 0.04253653 Acc 1.0\n",
      "Epoch 1650 Loss 0.034385677 Acc 1.0\n",
      "Epoch 1660 Loss 0.054345675 Acc 0.99\n",
      "Epoch 1670 Loss 0.053438015 Acc 1.0\n",
      "Epoch 1680 Loss 0.0495434 Acc 1.0\n",
      "Epoch 1690 Loss 0.040697288 Acc 1.0\n",
      "Epoch 1700 Loss 0.05459459 Acc 0.99\n",
      "Epoch 1710 Loss 0.049831897 Acc 0.99\n",
      "Epoch 1720 Loss 0.048154693 Acc 1.0\n",
      "Epoch 1730 Loss 0.0548907 Acc 0.99\n",
      "Epoch 1740 Loss 0.035974868 Acc 1.0\n",
      "Epoch 1750 Loss 0.055431534 Acc 1.0\n",
      "Epoch 1760 Loss 0.04431577 Acc 1.0\n",
      "Epoch 1770 Loss 0.037492182 Acc 1.0\n",
      "Epoch 1780 Loss 0.03727494 Acc 1.0\n",
      "Epoch 1790 Loss 0.045188274 Acc 1.0\n",
      "Epoch 1800 Loss 0.053581882 Acc 1.0\n",
      "Epoch 1810 Loss 0.054329056 Acc 1.0\n",
      "Epoch 1820 Loss 0.046984684 Acc 0.99\n",
      "Epoch 1830 Loss 0.033073816 Acc 1.0\n",
      "Epoch 1840 Loss 0.032178912 Acc 1.0\n",
      "Epoch 1850 Loss 0.036757838 Acc 1.0\n",
      "Epoch 1860 Loss 0.024312431 Acc 1.0\n",
      "Epoch 1870 Loss 0.037127905 Acc 1.0\n",
      "Epoch 1880 Loss 0.030549265 Acc 0.99\n",
      "Epoch 1890 Loss 0.05121902 Acc 0.98\n",
      "Epoch 1900 Loss 0.031311773 Acc 1.0\n",
      "Epoch 1910 Loss 0.03610712 Acc 1.0\n",
      "Epoch 1920 Loss 0.025008636 Acc 1.0\n",
      "Epoch 1930 Loss 0.029007798 Acc 1.0\n",
      "Epoch 1940 Loss 0.029206866 Acc 1.0\n",
      "Epoch 1950 Loss 0.02985369 Acc 1.0\n",
      "Epoch 1960 Loss 0.034698024 Acc 1.0\n",
      "Epoch 1970 Loss 0.033723827 Acc 1.0\n",
      "Epoch 1980 Loss 0.024749625 Acc 1.0\n",
      "Epoch 1990 Loss 0.023109451 Acc 1.0\n",
      "Test Review Data Shape:  (411, 150875)\n",
      "Final test accuracy: 0.9513382\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(initialize_all)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = [sample['x'] for sample in data]\n",
    "        labels  = [sample['y'] for sample in data]\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch\", epoch, \"Loss\", l, \"Acc\", acc)\n",
    "    random.shuffle(train)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_reviews = [sample['x'] for sample in test]\n",
    "test_labels  = [sample['y'] for sample in test]\n",
    "test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "print(\"Test Review Data Shape: \",np.array(test_reviews).shape)\n",
    "acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "print(\"Final test accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.4:  Encode each vocabulary word as a one-hot vector. Train RNN on the one-hot encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train/test datasets for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset:  1411\n",
      "# train:  1058\n",
      "# test:  353\n"
     ]
    }
   ],
   "source": [
    "### Choose Dataset\n",
    "with open(\"Data/movie-simple.txt\", \"r\", encoding=enc) as f:\n",
    "    dataset = [convert_line_to_example_onehot(l) for l in f.readlines()]  \n",
    "#with open(\"Data/movie-pang02.txt\", \"r\",encoding=enc) as f:\n",
    "#    dataset = [convert_line_to_example_onehot(l) for l in f.readlines()]\n",
    "\n",
    "print(\"Length of Dataset: \",len(dataset))\n",
    "\n",
    "# Split full dataset into train/test splits\n",
    "random.shuffle(dataset)\n",
    "batch_size = 1\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]\n",
    "\n",
    "print(\"# train: \", len(train))\n",
    "print(\"# test: \", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build RNN Model\n",
    "\n",
    "Note: Only running for 4 epochs because training is VERY VERY SLOW!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear old tf stuff\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Configs\n",
    "n_steps = None\n",
    "n_inputs = 150875\n",
    "n_neurons = 100\n",
    "num_epochs = 4\n",
    "\n",
    "# Input placeholders\n",
    "X= tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y= tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# Build RNN\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(n_neurons,activation=tf.nn.tanh)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "last_cell_output=outputs[:,-1,:]\n",
    "y_=tf.layers.dense(last_cell_output,1)\n",
    "\n",
    "# Loss and metrics\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_)), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train RNN on one-hot encoded movie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.4587483599366612 Acc 0.7884598111950616\n",
      "Epoch 1 Loss 0.16320897128847828 Acc 0.9130764524316867\n",
      "Epoch 2 Loss 0.07306027829956457 Acc 0.9831512704504691\n",
      "Epoch 3 Loss 0.10277702489273653 Acc 0.9690887780244798\n"
     ]
    }
   ],
   "source": [
    "initialize_all = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(initialize_all)\n",
    "l_ma=.74\n",
    "acc_ma=.5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = np.array([sample['w'] for sample in data]).reshape([1,-1,150875]) # New dims\n",
    "        labels  = np.array([sample['y'] for sample in data]).reshape([1,1])\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "        l_ma=.99*l_ma+(.01)*l\n",
    "        acc_ma=.99*acc_ma+(.01)*acc\n",
    "        #if (batch+1) % 100 == 0:\n",
    "        #    print(\"batch\", batch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Epoch\", epoch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
    "    random.shuffle(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test trained RNN on one-hot encoded test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.9320113314447592\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_acc=0\n",
    "n=0\n",
    "for sample in test:\n",
    "    test_reviews = np.array([sample['w'] ]).reshape([1,-1,150875])\n",
    "    test_labels  = np.array([sample['y']]).reshape([1,1])\n",
    "    test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "    test_acc += sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "    n+=1\n",
    "acc=test_acc/n \n",
    "print(\"Final test accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.5: Why did the word embeddings work better (hint: the word embeddings will work betterâ€¦)\n",
    "\n",
    "The word embeddings worked better because they are lower dimensions and it is intuitively simpler to learn the same task with less dimensions. It is favorable to have a higher ratio of training examples to number of input dimensions. In the case of the word embeddings the ratio is 1000 examples to 300 dimensions ($ratio>1$), while in the one hot embeddings the ratio is 1000 examples to 150875 dimensions ($ratio<1$). If we compare to Imagenet task, there are over 1,000,000 examples with about 50,000 dimensions, so $ratio>1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.6: How does cross-validation change when considering a time-series instead of multiple instances (as in our movie reviews)? Only a description is needed.\n",
    "\n",
    "In time series data, we may do cross-validation on data from the same sample. However, we may not do this with multiple instances. For example, we can split the time series data from a single sample into distinct parts and train on some and validate on others. If we have 3 seconds of audio data, we may train on the first two seconds and validate on the last second. Clearly, this will not work for multiple instances because we treat each instance as a distinct unit, not to be split up.\n",
    "\n",
    "We can use movie reviews as another example. With time series data we may only consider part of a movie review (i.e. 10 words) for training, and another part (i.e. 8 words) of that same review for validation. If the data is not time series, we must train and validate on data from completely different reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.7: In our previous homework assignment we considered the conditional GAN. In that case, the conditional label was known and given. Instead, consider generating images to match text. One approach could be to use an RNN to encode text to a vector that is fed to a conditional GAN (e.g. http://proceedings.mlr.press/v48/reed16.pdf). Draw a graph (but do not implement) how such a system could work. Any implementation here is completely optional, we are only looking for a description of how this could work.\n",
    "\n",
    "Refer to \"WrittenQuestions\" submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning: Homework 5 & 6\n",
    "\n",
    "**Nathan Inkawhich**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Duke Community Standard](http://integrity.duke.edu/standard.html): By typing your name below, you are certifying that you have adhered to the Duke Community Standard in completing this assignment.**\n",
    "\n",
    "Name: Nathan Inkawhich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: SARSA Q-Learning (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4a: Implement a deep Q-learning approach to the cart pole problem (was done in class) using an epsilon-Greedy approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] - Mean survival time over last 100 episodes was 11.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 30.68 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 57.62 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 76.54 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 94.31 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 76.39 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 120.25 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 145.88 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 186.74 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 176.81 ticks.\n",
      "Ran 919 episodes. Solved after 819 trials âœ”\n"
     ]
    }
   ],
   "source": [
    "# Based on: https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g/\n",
    "class DQNCartPoleSolver_QLearning():\n",
    "    def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/cartpole-1', force=True)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_win_ticks = n_win_ticks\n",
    "        self.batch_size = batch_size\n",
    "        self.quiet = quiet\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "\n",
    "        # Init model\n",
    "        self.state_ = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "        h = tf.layers.dense(self.state_, units=24, activation=tf.nn.tanh)\n",
    "        h = tf.layers.dense(h, units=48, activation=tf.nn.tanh)\n",
    "        self.Q = tf.layers.dense(h, units=2)\n",
    "        \n",
    "        self.Q_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "        loss = tf.losses.mean_squared_error(self.Q_, self.Q)\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        lr = tf.train.exponential_decay(0.01, self.global_step, 0.995, 1)\n",
    "        self.train_step = tf.train.AdamOptimizer(lr).minimize(loss, global_step=self.global_step)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.sess.run(self.Q, feed_dict={self.state_: state}))\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 4])\n",
    "\n",
    "    # Function to train Q network after every episode.\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        # Sample a minibatch of (state,action,reward,state',done) samples from memory. Here\n",
    "        #    memory is a queue of the last 100K iterations. Note, all of these samples are\n",
    "        #    not necessarily from the most recent episode but they are from recent episodes.\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        \n",
    "        # For each (state,action,reward,state',done) snapshot in the minibatch\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # Forward pass the state through Q network to get output scores of each action. \n",
    "            #    y_target = [[a1_score, a2_score]]\n",
    "            y_target = self.sess.run(self.Q, feed_dict={self.state_: state})\n",
    "            \n",
    "            # For the action we took, update the reward as either the reward from finishing\n",
    "            #    the game, or the reward plus the discounted future reward\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.sess.run(self.Q, feed_dict={self.state_: next_state})[0])\n",
    "            \n",
    "            # Format the inputs to the network so we can train in supervised way.\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "        \n",
    "        # Run a training step on this minibatch of sequences\n",
    "        self.sess.run(self.train_step, feed_dict={self.state_: np.array(x_batch), self.Q_: np.array(y_batch)})\n",
    "\n",
    "        # Decay the epsilon by a small amount because we just learned something. In beginning,\n",
    "        #    epsilon is high while the agent explores and at the end it should be low so \n",
    "        #    we can follow our \"optimal\" learned policy\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            # Run a full episode, meaning start a fresh game and have the agent\n",
    "            #    try to balance the pole\n",
    "            state = self.preprocess_state(self.env.reset())\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                # Uncomment to render graphics\n",
    "                #if e % 100 == 0 and not self.quiet:\n",
    "                #    self.env.render()\n",
    "                \n",
    "                # Step 1:\n",
    "                # Choose action with epsilon-greedy. Note, epsilon is decaying here.\n",
    "                #   In one case we just sample a random action from the action space and\n",
    "                #   in the other case we forward pass the state through self.Q and greedily\n",
    "                #   select the action with the largest value.\n",
    "                action = self.choose_action(state, self.get_epsilon(e))\n",
    "                \n",
    "                # Step 2:\n",
    "                # Take the action and observe the next state and reward\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # Step 3:\n",
    "                # Store this (state,action,reward,state',done) sequence into a memory buffer\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                \n",
    "                # Step 4: \n",
    "                # Perform state transition into next state\n",
    "                state = next_state\n",
    "                i += 1\n",
    "\n",
    "            # Record how many timesteps the agent was alive for. This is a buffer of len 100\n",
    "            #    so it only keeps the scores from the last 100 iterations\n",
    "            scores.append(i)\n",
    "            \n",
    "            # Average the scores from the last 100 episodes\n",
    "            mean_score = np.mean(scores)\n",
    "            \n",
    "            # Check for success\n",
    "            if mean_score >= self.n_win_ticks and e >= 100:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 100))\n",
    "                return e - 100\n",
    "            \n",
    "            # Output progress\n",
    "            if e % 100 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
    "            \n",
    "            # After every episode, train the Q network.\n",
    "            self.replay(self.batch_size)\n",
    "        \n",
    "        if not self.quiet: print('Did not solve after {} episodes ðŸ˜ž'.format(e))\n",
    "        return e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent = DQNCartPoleSolver_QLearning(n_episodes=1500)\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4b: Implement a SARSA approach with a deep network to solve the cart pole problem using an epsilon-Greedy approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] - Mean survival time over last 100 episodes was 13.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 23.55 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 54.33 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 180.84 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 186.0 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 194.64 ticks.\n",
      "Ran 518 episodes. Solved after 418 trials âœ”\n"
     ]
    }
   ],
   "source": [
    "# Based on: https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g/\n",
    "class DQNCartPoleSolver_SARSA():\n",
    "    def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/cartpole-1', force=True)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_win_ticks = n_win_ticks\n",
    "        self.batch_size = batch_size\n",
    "        self.quiet = quiet\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "\n",
    "        # Init model\n",
    "        self.state_ = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "        h = tf.layers.dense(self.state_, units=24, activation=tf.nn.tanh)\n",
    "        h = tf.layers.dense(h, units=48, activation=tf.nn.tanh)\n",
    "        self.Q = tf.layers.dense(h, units=2)\n",
    "        \n",
    "        self.Q_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "        loss = tf.losses.mean_squared_error(self.Q_, self.Q)\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        lr = tf.train.exponential_decay(0.01, self.global_step, 0.995, 1)\n",
    "        self.train_step = tf.train.AdamOptimizer(lr).minimize(loss, global_step=self.global_step)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, next_action, done):\n",
    "        self.memory.append((state, action, reward, next_state, next_action, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.sess.run(self.Q, feed_dict={self.state_: state}))\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 4])\n",
    "\n",
    "    # Function to train Q network after every episode.\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        # Sample a minibatch of (state,action,reward,state',done) samples from memory. Here\n",
    "        #    memory is a queue of the last 100K iterations. Note, all of these samples are\n",
    "        #    not necessarily from the most recent episode but they are from recent episodes.\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        \n",
    "        # For each (state,action,reward,state',done) snapshot in the minibatch\n",
    "        for state, action, reward, next_state, next_action, done in minibatch:\n",
    "            # Forward pass the state through Q network to get output scores of each action. \n",
    "            #    y_target = [[a1_score, a2_score]]\n",
    "            y_target = self.sess.run(self.Q, feed_dict={self.state_: state})\n",
    "            \n",
    "            # SARSA\n",
    "            # For the action we took, update the reward as either the reward from finishing\n",
    "            #    the game, or the reward plus the discounted future reward\n",
    "            #OLD: y_target[0][action] = reward if done else reward + self.gamma * np.max(self.sess.run(self.Q, feed_dict={self.state_: next_state})[0])\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * self.sess.run(self.Q, feed_dict={self.state_: next_state})[0][next_action]\n",
    "            \n",
    "            # Format the inputs to the network so we can train in supervised way.\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "        \n",
    "        # Run a training step on this minibatch of sequences\n",
    "        self.sess.run(self.train_step, feed_dict={self.state_: np.array(x_batch), self.Q_: np.array(y_batch)})\n",
    "\n",
    "        # Decay the epsilon by a small amount because we just learned something. In beginning,\n",
    "        #    epsilon is high while the agent explores and at the end it should be low so \n",
    "        #    we can follow our \"optimal\" learned policy\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            # Run a full episode, meaning start a fresh game and have the agent\n",
    "            #    try to balance the pole\n",
    "            state = self.preprocess_state(self.env.reset())\n",
    "            \n",
    "            # SARSA: Choose initial action to get started\n",
    "            action = self.choose_action(state, self.get_epsilon(e))\n",
    "            \n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                # Uncomment to render graphics\n",
    "                #if e % 100 == 0 and not self.quiet:\n",
    "                #    self.env.render()\n",
    "                \n",
    "                # Take the action and observe the next state and reward\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                \n",
    "                # SARSA: Choose next action based on next state. This is important because\n",
    "                #     Q-fxn will be updated with this info.\n",
    "                action_1 = self.choose_action(next_state, self.get_epsilon(e))\n",
    "                \n",
    "                # Store this (state,action,reward,state',action',done) sequence into a memory buffer\n",
    "                self.remember(state, action, reward, next_state, action_1, done)\n",
    "                \n",
    "                # Perform state transition into next state\n",
    "                state = next_state\n",
    "                action=action_1\n",
    "                i += 1\n",
    "\n",
    "            # Record how many timesteps the agent was alive for. This is a buffer of len 100\n",
    "            #    so it only keeps the scores from the last 100 iterations\n",
    "            scores.append(i)\n",
    "            \n",
    "            # Average the scores from the last 100 episodes\n",
    "            mean_score = np.mean(scores)\n",
    "            \n",
    "            # Check for success\n",
    "            if mean_score >= self.n_win_ticks and e >= 100:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 100))\n",
    "                return e - 100\n",
    "            \n",
    "            # Output progress\n",
    "            if e % 100 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
    "            \n",
    "            # After every episode, train the Q network.\n",
    "            self.replay(self.batch_size)\n",
    "        \n",
    "        if not self.quiet: print('Did not solve after {} episodes ðŸ˜ž'.format(e))\n",
    "        return e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent = DQNCartPoleSolver_SARSA(n_episodes=1500)\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4c: Evaluate the impact of $\\gamma$ in the cart pole problem. How important is this parameter? How does it affect stability and learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "gamma=0\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 12.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 27.94 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 9.46 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 9.58 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 11.06 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 11.87 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 11.56 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 14.06 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 14.69 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 15.87 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 10.96 ticks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 26.64 ticks.\n",
      "[Episode 1200] - Mean survival time over last 100 episodes was 14.16 ticks.\n",
      "[Episode 1300] - Mean survival time over last 100 episodes was 14.31 ticks.\n",
      "[Episode 1400] - Mean survival time over last 100 episodes was 21.17 ticks.\n",
      "[Episode 1500] - Mean survival time over last 100 episodes was 24.97 ticks.\n",
      "[Episode 1600] - Mean survival time over last 100 episodes was 24.11 ticks.\n",
      "[Episode 1700] - Mean survival time over last 100 episodes was 28.95 ticks.\n",
      "[Episode 1800] - Mean survival time over last 100 episodes was 31.1 ticks.\n",
      "[Episode 1900] - Mean survival time over last 100 episodes was 43.45 ticks.\n",
      "Did not solve after 1999 episodes ðŸ˜ž\n",
      "##################\n",
      "gamma=0.2\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 23.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 20.01 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 15.38 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 10.3 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 9.9 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 9.73 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 9.68 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 9.49 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 10.0 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 9.43 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 9.63 ticks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 9.45 ticks.\n",
      "[Episode 1200] - Mean survival time over last 100 episodes was 9.34 ticks.\n",
      "[Episode 1300] - Mean survival time over last 100 episodes was 9.47 ticks.\n",
      "[Episode 1400] - Mean survival time over last 100 episodes was 9.51 ticks.\n",
      "[Episode 1500] - Mean survival time over last 100 episodes was 9.46 ticks.\n",
      "[Episode 1600] - Mean survival time over last 100 episodes was 9.11 ticks.\n",
      "[Episode 1700] - Mean survival time over last 100 episodes was 9.17 ticks.\n",
      "[Episode 1800] - Mean survival time over last 100 episodes was 9.36 ticks.\n",
      "[Episode 1900] - Mean survival time over last 100 episodes was 9.39 ticks.\n",
      "Did not solve after 1999 episodes ðŸ˜ž\n",
      "##################\n",
      "gamma=0.4\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 34.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 24.48 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 14.01 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 13.56 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 33.98 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 46.56 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 28.37 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 41.59 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 47.2 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 58.98 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 44.59 ticks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 30.99 ticks.\n",
      "[Episode 1200] - Mean survival time over last 100 episodes was 48.12 ticks.\n",
      "[Episode 1300] - Mean survival time over last 100 episodes was 46.59 ticks.\n",
      "[Episode 1400] - Mean survival time over last 100 episodes was 60.75 ticks.\n",
      "[Episode 1500] - Mean survival time over last 100 episodes was 43.83 ticks.\n",
      "[Episode 1600] - Mean survival time over last 100 episodes was 39.78 ticks.\n",
      "[Episode 1700] - Mean survival time over last 100 episodes was 38.49 ticks.\n",
      "[Episode 1800] - Mean survival time over last 100 episodes was 40.41 ticks.\n",
      "[Episode 1900] - Mean survival time over last 100 episodes was 30.97 ticks.\n",
      "Did not solve after 1999 episodes ðŸ˜ž\n",
      "##################\n",
      "gamma=0.6\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 12.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 20.74 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 39.8 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 92.18 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 147.22 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 100.44 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 130.64 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 126.45 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 150.85 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 137.79 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 96.4 ticks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 150.16 ticks.\n",
      "[Episode 1200] - Mean survival time over last 100 episodes was 119.74 ticks.\n",
      "[Episode 1300] - Mean survival time over last 100 episodes was 99.8 ticks.\n",
      "[Episode 1400] - Mean survival time over last 100 episodes was 96.1 ticks.\n",
      "[Episode 1500] - Mean survival time over last 100 episodes was 73.17 ticks.\n",
      "[Episode 1600] - Mean survival time over last 100 episodes was 65.14 ticks.\n",
      "[Episode 1700] - Mean survival time over last 100 episodes was 77.77 ticks.\n",
      "[Episode 1800] - Mean survival time over last 100 episodes was 60.01 ticks.\n",
      "[Episode 1900] - Mean survival time over last 100 episodes was 102.89 ticks.\n",
      "Did not solve after 1999 episodes ðŸ˜ž\n",
      "##################\n",
      "gamma=0.8\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 15.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 69.99 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 19.22 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 81.94 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 123.02 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 138.39 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 145.95 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 147.81 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 109.41 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 137.5 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 140.37 ticks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 131.85 ticks.\n",
      "[Episode 1200] - Mean survival time over last 100 episodes was 115.7 ticks.\n",
      "[Episode 1300] - Mean survival time over last 100 episodes was 107.62 ticks.\n",
      "[Episode 1400] - Mean survival time over last 100 episodes was 125.58 ticks.\n",
      "[Episode 1500] - Mean survival time over last 100 episodes was 132.81 ticks.\n",
      "[Episode 1600] - Mean survival time over last 100 episodes was 120.03 ticks.\n",
      "[Episode 1700] - Mean survival time over last 100 episodes was 146.17 ticks.\n",
      "[Episode 1800] - Mean survival time over last 100 episodes was 121.78 ticks.\n",
      "[Episode 1900] - Mean survival time over last 100 episodes was 88.83 ticks.\n",
      "Did not solve after 1999 episodes ðŸ˜ž\n",
      "##################\n",
      "gamma=1.0\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 12.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 9.82 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 38.25 ticks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 300] - Mean survival time over last 100 episodes was 106.26 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 105.88 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 119.61 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 119.06 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 116.43 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 165.1 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 159.63 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 100.44 ticks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 102.21 ticks.\n",
      "Ran 1197 episodes. Solved after 1097 trials âœ”\n"
     ]
    }
   ],
   "source": [
    "for gamma in [0, .2, .4, .6, .8, 1.]:\n",
    "    print(\"##################\\ngamma={}\\n##################\\n\".format(gamma))\n",
    "    agent = DQNCartPoleSolver_QLearning(n_episodes=2000, gamma=gamma)\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "SARSA gamma=0\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 22.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 14.54 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 9.42 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 9.22 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 9.36 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 9.48 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 9.47 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 10.69 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 10.14 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 9.96 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 9.54 ticks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 9.88 ticks.\n",
      "[Episode 1200] - Mean survival time over last 100 episodes was 11.49 ticks.\n",
      "[Episode 1300] - Mean survival time over last 100 episodes was 11.57 ticks.\n",
      "[Episode 1400] - Mean survival time over last 100 episodes was 11.67 ticks.\n",
      "[Episode 1500] - Mean survival time over last 100 episodes was 9.42 ticks.\n",
      "[Episode 1600] - Mean survival time over last 100 episodes was 9.99 ticks.\n",
      "[Episode 1700] - Mean survival time over last 100 episodes was 9.98 ticks.\n",
      "[Episode 1800] - Mean survival time over last 100 episodes was 9.44 ticks.\n",
      "[Episode 1900] - Mean survival time over last 100 episodes was 9.33 ticks.\n",
      "Did not solve after 1999 episodes ðŸ˜ž\n",
      "##################\n",
      "SARSA gamma=0.2\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 19.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 17.79 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 21.51 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 23.41 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 25.31 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 32.86 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 39.92 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 36.81 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 13.48 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 11.28 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 26.88 ticks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 21.59 ticks.\n",
      "[Episode 1200] - Mean survival time over last 100 episodes was 17.79 ticks.\n",
      "[Episode 1300] - Mean survival time over last 100 episodes was 14.31 ticks.\n",
      "[Episode 1400] - Mean survival time over last 100 episodes was 15.29 ticks.\n",
      "[Episode 1500] - Mean survival time over last 100 episodes was 21.53 ticks.\n",
      "[Episode 1600] - Mean survival time over last 100 episodes was 11.43 ticks.\n",
      "[Episode 1700] - Mean survival time over last 100 episodes was 17.34 ticks.\n",
      "[Episode 1800] - Mean survival time over last 100 episodes was 12.47 ticks.\n",
      "[Episode 1900] - Mean survival time over last 100 episodes was 9.76 ticks.\n",
      "Did not solve after 1999 episodes ðŸ˜ž\n",
      "##################\n",
      "SARSA gamma=0.4\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 12.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 12.06 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 12.13 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 11.58 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 11.51 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 14.24 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 16.91 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 23.3 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 16.27 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 15.13 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 31.85 ticks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 42.2 ticks.\n",
      "[Episode 1200] - Mean survival time over last 100 episodes was 28.99 ticks.\n",
      "[Episode 1300] - Mean survival time over last 100 episodes was 18.08 ticks.\n",
      "[Episode 1400] - Mean survival time over last 100 episodes was 33.12 ticks.\n",
      "[Episode 1500] - Mean survival time over last 100 episodes was 44.39 ticks.\n",
      "[Episode 1600] - Mean survival time over last 100 episodes was 64.26 ticks.\n",
      "[Episode 1700] - Mean survival time over last 100 episodes was 77.64 ticks.\n",
      "[Episode 1800] - Mean survival time over last 100 episodes was 122.99 ticks.\n",
      "[Episode 1900] - Mean survival time over last 100 episodes was 79.9 ticks.\n",
      "Did not solve after 1999 episodes ðŸ˜ž\n",
      "##################\n",
      "SARSA gamma=0.6\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 17.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 17.09 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 26.69 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 51.09 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 95.28 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 112.83 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 112.18 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 111.97 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 133.45 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 145.75 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 145.48 ticks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 135.0 ticks.\n",
      "[Episode 1200] - Mean survival time over last 100 episodes was 154.96 ticks.\n",
      "[Episode 1300] - Mean survival time over last 100 episodes was 147.6 ticks.\n",
      "[Episode 1400] - Mean survival time over last 100 episodes was 141.56 ticks.\n",
      "[Episode 1500] - Mean survival time over last 100 episodes was 137.5 ticks.\n",
      "[Episode 1600] - Mean survival time over last 100 episodes was 128.35 ticks.\n",
      "[Episode 1700] - Mean survival time over last 100 episodes was 109.85 ticks.\n",
      "[Episode 1800] - Mean survival time over last 100 episodes was 152.23 ticks.\n",
      "[Episode 1900] - Mean survival time over last 100 episodes was 135.38 ticks.\n",
      "Did not solve after 1999 episodes ðŸ˜ž\n",
      "##################\n",
      "SARSA gamma=0.8\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 34.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 33.56 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 114.07 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 175.78 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 154.56 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 181.08 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 193.42 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 164.4 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 191.84 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 169.11 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 163.42 ticks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 157.99 ticks.\n",
      "[Episode 1200] - Mean survival time over last 100 episodes was 160.17 ticks.\n",
      "[Episode 1300] - Mean survival time over last 100 episodes was 153.71 ticks.\n",
      "[Episode 1400] - Mean survival time over last 100 episodes was 139.01 ticks.\n",
      "[Episode 1500] - Mean survival time over last 100 episodes was 97.96 ticks.\n",
      "[Episode 1600] - Mean survival time over last 100 episodes was 86.41 ticks.\n",
      "[Episode 1700] - Mean survival time over last 100 episodes was 101.54 ticks.\n",
      "[Episode 1800] - Mean survival time over last 100 episodes was 127.36 ticks.\n",
      "[Episode 1900] - Mean survival time over last 100 episodes was 134.46 ticks.\n",
      "Did not solve after 1999 episodes ðŸ˜ž\n",
      "##################\n",
      "SARSA gamma=1.0\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 17.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 51.13 ticks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 200] - Mean survival time over last 100 episodes was 145.55 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 123.39 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 169.35 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 174.99 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 185.78 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 193.16 ticks.\n",
      "Ran 714 episodes. Solved after 614 trials âœ”\n"
     ]
    }
   ],
   "source": [
    "for gamma in [0, .2, .4, .6, .8, 1.]:\n",
    "    print(\"##################\\nSARSA gamma={}\\n##################\\n\".format(gamma))\n",
    "    agent = DQNCartPoleSolver_SARSA(n_episodes=2000, gamma=gamma)\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4d: Evaluate the impact of $\\epsilon$ on the learning over the feasible range (0-1). What values seem reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "epsilon=0.0\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 10.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 9.2 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 9.37 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 9.44 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 9.39 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 9.32 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 9.35 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 9.49 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 53.19 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 43.38 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 91.45 ticks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 103.22 ticks.\n",
      "[Episode 1200] - Mean survival time over last 100 episodes was 145.01 ticks.\n",
      "[Episode 1300] - Mean survival time over last 100 episodes was 162.65 ticks.\n",
      "[Episode 1400] - Mean survival time over last 100 episodes was 127.58 ticks.\n",
      "Did not solve after 1499 episodes ðŸ˜ž\n",
      "##################\n",
      "epsilon=0.2\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 54.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 51.67 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 57.1 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 110.07 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 129.43 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 72.5 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 96.91 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 103.56 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 123.08 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 146.43 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 134.86 ticks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 154.49 ticks.\n",
      "[Episode 1200] - Mean survival time over last 100 episodes was 165.39 ticks.\n",
      "[Episode 1300] - Mean survival time over last 100 episodes was 162.36 ticks.\n",
      "[Episode 1400] - Mean survival time over last 100 episodes was 161.71 ticks.\n",
      "Did not solve after 1499 episodes ðŸ˜ž\n",
      "##################\n",
      "epsilon=0.4\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 12.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 27.51 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 89.37 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 96.43 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 118.35 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 131.17 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 135.87 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 128.15 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 158.59 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 142.27 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 142.98 ticks.\n",
      "Ran 1095 episodes. Solved after 995 trials âœ”\n",
      "##################\n",
      "epsilon=0.6\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 73.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 33.0 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 77.01 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 100.59 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 129.03 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 150.69 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 145.37 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 95.73 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 141.4 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 190.22 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 124.25 ticks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 157.59 ticks.\n",
      "Ran 1150 episodes. Solved after 1050 trials âœ”\n",
      "##################\n",
      "epsilon=0.8\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 39.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 22.55 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 78.7 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 100.79 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 100.69 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 119.58 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 92.3 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 154.23 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 122.0 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 161.86 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 132.64 ticks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 157.18 ticks.\n",
      "[Episode 1200] - Mean survival time over last 100 episodes was 171.13 ticks.\n",
      "[Episode 1300] - Mean survival time over last 100 episodes was 174.9 ticks.\n",
      "[Episode 1400] - Mean survival time over last 100 episodes was 179.07 ticks.\n",
      "Ran 1465 episodes. Solved after 1365 trials âœ”\n",
      "##################\n",
      "epsilon=1.0\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 22.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 22.97 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 74.85 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 87.86 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 96.82 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 99.19 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 156.72 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 134.52 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 142.09 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 146.59 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 136.46 ticks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 134.0 ticks.\n",
      "[Episode 1200] - Mean survival time over last 100 episodes was 163.03 ticks.\n",
      "Ran 1280 episodes. Solved after 1180 trials âœ”\n"
     ]
    }
   ],
   "source": [
    "for eps in [0., .2, .4, .6, .8, 1.]:\n",
    "    print(\"##################\\nepsilon={}\\n##################\\n\".format(eps))\n",
    "    agent = DQNCartPoleSolver_QLearning(n_episodes=1500, epsilon=eps, epsilon_min=0.0, epsilon_log_decay=0.995)\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "SARSA epsilon=0.0\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 200.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 69.85 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 107.97 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 57.7 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 87.47 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 180.38 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 184.75 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 181.93 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 180.48 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 168.03 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 132.44 ticks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 140.11 ticks.\n",
      "[Episode 1200] - Mean survival time over last 100 episodes was 165.95 ticks.\n",
      "[Episode 1300] - Mean survival time over last 100 episodes was 161.96 ticks.\n",
      "Ran 1343 episodes. Solved after 1243 trials âœ”\n",
      "##################\n",
      "SARSA epsilon=0.2\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 48.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 16.88 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 131.52 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 180.07 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 183.32 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 174.09 ticks.\n",
      "Ran 548 episodes. Solved after 448 trials âœ”\n",
      "##################\n",
      "SARSA epsilon=0.4\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 83.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 40.04 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 76.08 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 150.48 ticks.\n",
      "Ran 378 episodes. Solved after 278 trials âœ”\n",
      "##################\n",
      "SARSA epsilon=0.6\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 35.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 23.99 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 80.89 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 151.06 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 194.25 ticks.\n",
      "Ran 404 episodes. Solved after 304 trials âœ”\n",
      "##################\n",
      "SARSA epsilon=0.8\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 15.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 32.77 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 109.12 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 192.15 ticks.\n",
      "Ran 310 episodes. Solved after 210 trials âœ”\n",
      "##################\n",
      "SARSA epsilon=1.0\n",
      "##################\n",
      "\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 28.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 40.16 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 70.89 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 158.35 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 185.76 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 190.51 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 189.46 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 159.23 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 159.91 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 144.43 ticks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 175.85 ticks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 174.81 ticks.\n",
      "Ran 1163 episodes. Solved after 1063 trials âœ”\n"
     ]
    }
   ],
   "source": [
    "for eps in [0., .2, .4, .6, .8, 1.]:\n",
    "    print(\"##################\\nSARSA epsilon={}\\n##################\\n\".format(eps))\n",
    "    agent = DQNCartPoleSolver_SARSA(n_episodes=1500, epsilon=eps, epsilon_min=0.0, epsilon_log_decay=0.995)\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4e: Pick one other small agent from the OpenAI gym and apply the same techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#env = gym.make(\"Acrobot-v1\")\n",
    "#env = gym.make(\"Pong-v0\")\n",
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(2,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4e: Q-Learning Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[Episode 0] - Mean/Max score -0.4474913144874446/-0.4474913144874446. Num wins: 0. eps = 0.9. lr = 0.001\n",
      "[Episode 20] - Mean/Max score -0.34452924208861385/-0.13733566059288155. Num wins: 0. eps = 0.8821699783465812. lr = 0.001\n",
      "[Episode 40] - Mean/Max score -0.36000046946886083/-0.13733566059288155. Num wins: 0. eps = 0.8646931896622304. lr = 0.001\n",
      "[Episode 60] - Mean/Max score -0.35475116904265913/-0.13733566059288155. Num wins: 0. eps = 0.8475626360008509. lr = 0.001\n",
      "[Episode 80] - Mean/Max score -0.3584701600248111/-0.13733566059288155. Num wins: 0. eps = 0.8307714580536021. lr = 0.001\n",
      "[Episode 100] - Mean/Max score -0.3521744009452288/-0.13733566059288155. Num wins: 0. eps = 0.8143129324023378. lr = 0.001\n",
      "[Episode 120] - Mean/Max score -0.35871806450134386/-0.15023361034718635. Num wins: 0. eps = 0.7981804688274572. lr = 0.001\n",
      "[Episode 140] - Mean/Max score -0.36210102103765623/-0.15023361034718635. Num wins: 0. eps = 0.7823676076690911. lr = 0.001\n",
      "[Episode 160] - Mean/Max score -0.3670284680763558/-0.22976015520344079. Num wins: 0. eps = 0.766868017240565. lr = 0.001\n",
      "[Episode 180] - Mean/Max score -0.3564897115664825/-0.16007277523842245. Num wins: 0. eps = 0.7516754912931053. lr = 0.001\n",
      "[Episode 200] - Mean/Max score -0.36111310386923423/-0.16007277523842245. Num wins: 0. eps = 0.7367839465307715. lr = 0.001\n",
      "[Episode 220] - Mean/Max score -0.3612338602855621/-0.16007277523842245. Num wins: 0. eps = 0.7221874201746215. lr = 0.001\n",
      "[Episode 240] - Mean/Max score -0.3567969026986529/-0.16007277523842245. Num wins: 0. eps = 0.7078800675751326. lr = 0.001\n",
      "[Episode 260] - Mean/Max score -0.35942020200930935/-0.16007277523842245. Num wins: 0. eps = 0.6938561598719233. lr = 0.001\n",
      "[Episode 280] - Mean/Max score -0.3734599073775666/-0.19211664105569146. Num wins: 0. eps = 0.6801100816998408. lr = 0.001\n",
      "[Episode 300] - Mean/Max score -0.3853173655049118/-0.2016469889395214. Num wins: 0. eps = 0.666636328940489. lr = 0.001\n",
      "[Episode 320] - Mean/Max score -0.39496314827101003/-0.1816320426364652. Num wins: 0. eps = 0.6534295065183061. lr = 0.001\n",
      "[Episode 340] - Mean/Max score -0.40636298403469673/-0.1816320426364652. Num wins: 0. eps = 0.6404843262403014. lr = 0.001\n",
      "[Episode 360] - Mean/Max score -0.37676833000693916/0.04195923890383525. Num wins: 0. eps = 0.6277956046785902. lr = 0.001\n",
      "[Episode 380] - Mean/Max score -0.33576970435810305/0.08347490083157515. Num wins: 0. eps = 0.6153582610948785. lr = 0.001\n",
      "[Episode 400] - Mean/Max score -0.25890496200684837/0.3328899580445074. Num wins: 0. eps = 0.6031673154060654. lr = 0.001\n",
      "[Episode 420] - Mean/Max score -0.1981036205894736/0.3328899580445074. Num wins: 0. eps = 0.5912178861901491. lr = 0.001\n",
      "[Episode 440] - Mean/Max score -0.10810074773090537/0.3328899580445074. Num wins: 0. eps = 0.5795051887316394. lr = 0.001\n",
      "Ran 454 episodes. Solved after 354 trials âœ”\n"
     ]
    }
   ],
   "source": [
    "class DQNMountainCarSolver():\n",
    "    def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.1, epsilon_log_decay=0.99, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.env = gym.make('MountainCar-v0')\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/mountaincar-1', force=True)\n",
    "        self.gamma = gamma\n",
    "        self.lr = 0.001\n",
    "        self.epsilon = epsilon\n",
    "        #self.epsilon = 0.3\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_win_ticks = n_win_ticks\n",
    "        self.batch_size = batch_size\n",
    "        self.quiet = quiet\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "\n",
    "        # Init Q model\n",
    "        self.state_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "        h = tf.layers.dense(self.state_, units=50, activation=tf.nn.tanh, kernel_initializer=tf.truncated_normal_initializer)\n",
    "        #h = tf.layers.dense(self.state_, units=100, activation=None,kernel_initializer=tf.truncated_normal_initializer)\n",
    "        #h = tf.layers.dense(self.state_, units=50, activation=tf.nn.tanh,kernel_initializer=tf.truncated_normal_initializer)\n",
    "        #h = tf.layers.dense(h, units=50, activation=tf.nn.tanh)\n",
    "        self.Q = tf.layers.dense(h, units=3)\n",
    "        \n",
    "        self.Q_ = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "        loss = tf.losses.mean_squared_error(self.Q_, self.Q)\n",
    "        #self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        #lr = tf.train.exponential_decay(0.001, self.global_step, 0.995, 1)\n",
    "        #self.train_step = tf.train.AdamOptimizer().minimize(loss, global_step=self.global_step)\n",
    "        self.learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "        self.train_step = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.sess.run(self.Q, feed_dict={self.state_: state, self.learning_rate: self.lr}))\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        #return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "        return self.epsilon\n",
    "    \n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 2])\n",
    "\n",
    "    # Function to train Q network after every episode.\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        # Sample a minibatch of (state,action,reward,state',done) samples from memory. Here\n",
    "        #    memory is a queue of the last 100K iterations. Note, all of these samples are\n",
    "        #    not necessarily from the most recent episode but they are from recent episodes.\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        \n",
    "        # For each (state,action,reward,state',done) snapshot in the minibatch\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # Forward pass the state through Q network to get output scores of each action. \n",
    "            y_target = self.sess.run(self.Q, feed_dict={self.state_: state, self.learning_rate: self.lr})\n",
    "            \n",
    "            # For the action we took, update the reward as either the reward from finishing\n",
    "            #    the game, or the reward plus the discounted future reward\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.sess.run(self.Q, feed_dict={self.state_: next_state, self.learning_rate: self.lr})[0])\n",
    "            \n",
    "            # Format the inputs to the network so we can train in supervised way.\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "        \n",
    "        # Run a training step on this minibatch of sequences\n",
    "        self.sess.run(self.train_step, feed_dict={self.state_: np.array(x_batch), self.Q_: np.array(y_batch), self.learning_rate: self.lr})\n",
    "\n",
    "        # Decay the epsilon by a small amount because we just learned something. In beginning,\n",
    "        #    epsilon is high while the agent explores and at the end it should be low so \n",
    "        #    we can follow our \"optimal\" learned policy\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "        num_wins = 0\n",
    "        steps = 300\n",
    "        \n",
    "        for e in range(self.n_episodes):\n",
    "        #for e in range(1):\n",
    "            # Run a full episode, meaning start a fresh game and have the agent\n",
    "            #    try to balance the pole\n",
    "            state = self.preprocess_state(self.env.reset())\n",
    "            done = False\n",
    "            i = 0\n",
    "            max_pos = -100\n",
    "            \n",
    "            #while not done:\n",
    "            for s in range(steps):\n",
    "                # Uncomment to render graphics\n",
    "                #if e % 100 == 0 and not self.quiet:\n",
    "                #    self.env.render()\n",
    "                \n",
    "                # Step 1:\n",
    "                # Choose action with epsilon-greedy. Note, epsilon is decaying here.\n",
    "                #   In one case we just sample a random action from the action space and\n",
    "                #   in the other case we forward pass the state through self.Q and greedily\n",
    "                #   select the action with the largest value.\n",
    "                action = self.choose_action(state, self.get_epsilon(e))\n",
    "                \n",
    "                # Step 2:\n",
    "                # Take the action and observe the next state and reward\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # Step 2.1: Modify reward signal to encourage faster learning.\n",
    "                #    Here we adjust reward based on car position and for task completion.\n",
    "                #    Note, next_state[0] is the car's position. Inspired by\n",
    "                #    https://medium.com/@ts1829/solving-mountain-car-with-q-learning-b77bf71b1de2\n",
    "                #print(\"Next state: {}; reward: {}; done: {}\".format(next_state,reward,done))\n",
    "                reward = next_state[0] - 0.5\n",
    "                if next_state[0] >= 0.5:\n",
    "                    reward += 1\n",
    "                    \n",
    "                # update best position seen\n",
    "                if next_state[0] >= max_pos:\n",
    "                    max_pos = next_state[0]\n",
    "                \n",
    "                # if done\n",
    "                if done:\n",
    "                    # if we have ended in a success, we will update some learning hyperparams\n",
    "                    if next_state[0] >= 0.5:\n",
    "                        # Decay epsilon\n",
    "                        self.epsilon *= 0.95\n",
    "                        # Decay learning rate\n",
    "                        self.lr *= 0.9\n",
    "                        num_wins += 1\n",
    "                \n",
    "                # Step 3:\n",
    "                # Store this (state,action,reward,state',done) sequence into a memory buffer\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                \n",
    "                # Step 4: \n",
    "                # Perform state transition into next state\n",
    "                state = next_state\n",
    "                i += 1\n",
    "                \n",
    "            #\"\"\"\n",
    "            # Record the final position of the cart. This is a buffer of len 100\n",
    "            #    so it only keeps the positions from the last 100 iterations\n",
    "            #scores.append(next_state[0])\n",
    "            scores.append(max_pos)\n",
    "            \n",
    "            # Average the scores from the last 100 episodes\n",
    "            mean_score = np.mean(scores)\n",
    "            max_score = np.max(scores)\n",
    "            \n",
    "            # Check for success\n",
    "            if max_score >= 0.5 and e >= 100:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 100))\n",
    "                return e - 100\n",
    "            \n",
    "            # Output progress\n",
    "            if e % 20 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean/Max score {}/{}. Num wins: {}. eps = {}. lr = {}'.format(e, mean_score,max_score, num_wins,self.epsilon,self.lr))\n",
    "            \n",
    "            # After every episode, train the Q network.\n",
    "            self.replay(self.batch_size)\n",
    "            #\"\"\"\n",
    "        \n",
    "        if not self.quiet: print('Did not solve after {} episodes ðŸ˜ž'.format(e))\n",
    "        return e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent = DQNMountainCarSolver(n_episodes=3000,gamma=.99,epsilon=0.9, epsilon_min=0.01, epsilon_log_decay=0.999)\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4e: SARSA Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[Episode 0] - Mean/Max score -0.47125564781239215/-0.47125564781239215. Num wins: 0. eps = 0.9. lr = 0.001\n",
      "[Episode 20] - Mean/Max score -0.3684692801096171/-0.21411975592712723. Num wins: 0. eps = 0.8821699783465812. lr = 0.001\n",
      "[Episode 40] - Mean/Max score -0.3610644160333316/-0.21411975592712723. Num wins: 0. eps = 0.8646931896622304. lr = 0.001\n",
      "[Episode 60] - Mean/Max score -0.36756389012880075/-0.21411975592712723. Num wins: 0. eps = 0.8475626360008509. lr = 0.001\n",
      "[Episode 80] - Mean/Max score -0.3737211393262115/-0.21411975592712723. Num wins: 0. eps = 0.8307714580536021. lr = 0.001\n",
      "[Episode 100] - Mean/Max score -0.36371559610665094/-0.09351564248902206. Num wins: 0. eps = 0.8143129324023378. lr = 0.001\n",
      "[Episode 120] - Mean/Max score -0.35821296849413914/-0.07175527200867365. Num wins: 0. eps = 0.7981804688274572. lr = 0.001\n",
      "[Episode 140] - Mean/Max score -0.3149887485762749/0.05645785505836488. Num wins: 0. eps = 0.7823676076690911. lr = 0.001\n",
      "[Episode 160] - Mean/Max score -0.268862293465705/0.10146459663293367. Num wins: 0. eps = 0.766868017240565. lr = 0.001\n",
      "[Episode 180] - Mean/Max score -0.2094854003226485/0.2589594195842011. Num wins: 0. eps = 0.7516754912931053. lr = 0.001\n",
      "[Episode 200] - Mean/Max score -0.15046744391944278/0.2589594195842011. Num wins: 0. eps = 0.7367839465307715. lr = 0.001\n",
      "[Episode 220] - Mean/Max score -0.0895173065624266/0.2589594195842011. Num wins: 0. eps = 0.7221874201746215. lr = 0.001\n",
      "[Episode 240] - Mean/Max score -0.07104114458020647/0.2589594195842011. Num wins: 0. eps = 0.7078800675751326. lr = 0.001\n",
      "[Episode 260] - Mean/Max score -0.022544108413430675/0.4780628677829827. Num wins: 0. eps = 0.6938561598719233. lr = 0.001\n",
      "Ran 273 episodes. Solved after 173 trials âœ”\n"
     ]
    }
   ],
   "source": [
    "class DQNMountainCarSolver_SARSA():\n",
    "    def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.1, epsilon_log_decay=0.99, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.env = gym.make('MountainCar-v0')\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/mountaincar-1', force=True)\n",
    "        self.gamma = gamma\n",
    "        self.lr = 0.001\n",
    "        self.epsilon = epsilon\n",
    "        #self.epsilon = 0.3\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_win_ticks = n_win_ticks\n",
    "        self.batch_size = batch_size\n",
    "        self.quiet = quiet\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "\n",
    "        # Init Q model\n",
    "        self.state_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "        h = tf.layers.dense(self.state_, units=50, activation=tf.nn.tanh, kernel_initializer=tf.truncated_normal_initializer)\n",
    "        #h = tf.layers.dense(self.state_, units=100, activation=None,kernel_initializer=tf.truncated_normal_initializer)\n",
    "        #h = tf.layers.dense(self.state_, units=50, activation=tf.nn.tanh,kernel_initializer=tf.truncated_normal_initializer)\n",
    "        #h = tf.layers.dense(h, units=50, activation=tf.nn.tanh)\n",
    "        self.Q = tf.layers.dense(h, units=3)\n",
    "        \n",
    "        self.Q_ = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "        loss = tf.losses.mean_squared_error(self.Q_, self.Q)\n",
    "        #self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        #lr = tf.train.exponential_decay(0.001, self.global_step, 0.995, 1)\n",
    "        #self.train_step = tf.train.AdamOptimizer().minimize(loss, global_step=self.global_step)\n",
    "        self.learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "        self.train_step = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, next_action, done):\n",
    "        self.memory.append((state, action, reward, next_state, next_action, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.sess.run(self.Q, feed_dict={self.state_: state, self.learning_rate: self.lr}))\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        #return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "        return self.epsilon\n",
    "    \n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 2])\n",
    "\n",
    "    # Function to train Q network after every episode.\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        # Sample a minibatch of (state,action,reward,state',done) samples from memory. Here\n",
    "        #    memory is a queue of the last 100K iterations. Note, all of these samples are\n",
    "        #    not necessarily from the most recent episode but they are from recent episodes.\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        \n",
    "        # For each (state,action,reward,state',done) snapshot in the minibatch\n",
    "        for state, action, reward, next_state, next_action, done in minibatch:\n",
    "            # Forward pass the state through Q network to get output scores of each action. \n",
    "            y_target = self.sess.run(self.Q, feed_dict={self.state_: state, self.learning_rate: self.lr})\n",
    "            \n",
    "            # SARSA\n",
    "            # For the action we took, update the reward as either the reward from finishing\n",
    "            #    the game, or the reward plus the discounted future reward\n",
    "            # OLD: y_target[0][action] = reward if done else reward + self.gamma * np.max(self.sess.run(self.Q, feed_dict={self.state_: next_state, self.learning_rate: self.lr})[0])\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * self.sess.run(self.Q, feed_dict={self.state_: next_state, self.learning_rate: self.lr})[0][next_action]\n",
    "            \n",
    "            # Format the inputs to the network so we can train in supervised way.\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "        \n",
    "        # Run a training step on this minibatch of sequences\n",
    "        self.sess.run(self.train_step, feed_dict={self.state_: np.array(x_batch), self.Q_: np.array(y_batch), self.learning_rate: self.lr})\n",
    "\n",
    "        # Decay the epsilon by a small amount because we just learned something. In beginning,\n",
    "        #    epsilon is high while the agent explores and at the end it should be low so \n",
    "        #    we can follow our \"optimal\" learned policy\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "        num_wins = 0\n",
    "        steps = 300\n",
    "        \n",
    "        for e in range(self.n_episodes):\n",
    "            \n",
    "            # Pick an initial state and action\n",
    "            state = self.preprocess_state(self.env.reset())\n",
    "            action = self.choose_action(state, self.get_epsilon(e))\n",
    "                \n",
    "            done = False\n",
    "            i = 0\n",
    "            max_pos = -100\n",
    "            \n",
    "            for s in range(steps):\n",
    "                # Uncomment to render graphics\n",
    "                #if e % 100 == 0 and not self.quiet:\n",
    "                #    self.env.render()\n",
    "                \n",
    "                # Take the action and observe the next state and reward\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # Modify reward signal to encourage faster learning.\n",
    "                #    Here we adjust reward based on car position and for task completion.\n",
    "                #    Note, next_state[0] is the car's position. Inspired by\n",
    "                #    https://medium.com/@ts1829/solving-mountain-car-with-q-learning-b77bf71b1de2\n",
    "                #print(\"Next state: {}; reward: {}; done: {}\".format(next_state,reward,done))\n",
    "                reward = next_state[0] - 0.5\n",
    "                if next_state[0] >= 0.5:\n",
    "                    reward += 1\n",
    "                    \n",
    "                # update best position seen\n",
    "                if next_state[0] >= max_pos:\n",
    "                    max_pos = next_state[0]\n",
    "                \n",
    "                # if done\n",
    "                if done:\n",
    "                    # if we have ended in a success, we will update some learning hyperparams\n",
    "                    if next_state[0] >= 0.5:\n",
    "                        # Decay epsilon\n",
    "                        self.epsilon *= 0.95\n",
    "                        # Decay learning rate\n",
    "                        self.lr *= 0.9\n",
    "                        num_wins += 1\n",
    "                \n",
    "                # Using next state pick next action for SARSA\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                next_action = self.choose_action(next_state, self.get_epsilon(e))\n",
    "                \n",
    "                # Store this (state,action,reward,state',done) sequence into a memory buffer\n",
    "                self.remember(state, action, reward, next_state, next_action, done)\n",
    "                \n",
    "                # Step 4: \n",
    "                # Perform state transition into next state\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                i += 1\n",
    "                \n",
    "            #\"\"\"\n",
    "            # Record the final position of the cart. This is a buffer of len 100\n",
    "            #    so it only keeps the positions from the last 100 iterations\n",
    "            #scores.append(next_state[0])\n",
    "            scores.append(max_pos)\n",
    "            \n",
    "            # Average the scores from the last 100 episodes\n",
    "            mean_score = np.mean(scores)\n",
    "            max_score = np.max(scores)\n",
    "            \n",
    "            # Check for success\n",
    "            if max_score >= 0.5 and e >= 100:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 100))\n",
    "                return e - 100\n",
    "            \n",
    "            # Output progress\n",
    "            if e % 20 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean/Max score {}/{}. Num wins: {}. eps = {}. lr = {}'.format(e, mean_score,max_score, num_wins,self.epsilon,self.lr))\n",
    "            \n",
    "            # After every episode, train the Q network.\n",
    "            self.replay(self.batch_size)\n",
    "            #\"\"\"\n",
    "        \n",
    "        if not self.quiet: print('Did not solve after {} episodes ðŸ˜ž'.format(e))\n",
    "        return e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent = DQNMountainCarSolver_SARSA(n_episodes=3000,gamma=.99,epsilon=0.9, epsilon_min=0.01, epsilon_log_decay=0.999)\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
